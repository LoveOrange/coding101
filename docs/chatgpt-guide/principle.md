---
sidebar_position: 2
---

# ChatGPT 以及 LLM 原理浅析

ChatGPT 是一种大语言模型（Large language models，简称 LLM），而 LLM 又是一种预测型模型。

所谓的预测型模型，简单来说就是接话茬，在给定上文的接触上，预测接下来最可能的下文是什么。

为了实现这一点，需要在训练 LLM 的过程中提供超巨量的数据，LLM 会学习语言的基础知识、语法规则、事实知识和语言之间的关联等，这一步被称为“预训练”。在 LLM 掌握了基本的语言知识后，还可以提供给 LLM 特定的训练素材，帮助 LLM 更好的理解特定的场景和需求，这一步被称为“微调”。

举个例子，我们想要训练一个爱吃苹果的 LLM。首先，我们先把我们能找到的所有中文素材，经过格式化之后喂给 LLM，帮助它学习中文的语法和知识，得到预训练模型。接下来，我们用特定的数据对它进行微调。我们不断地告诉与训练模型：“苹果好吃”、“吃苹果健康”、“我爱吃苹果”。那么当我们问模型什么水果好吃的时候，大模型就倾向于做一个苹果推广大使。

当然这只是一个适用范围极小的场景。如果要训练 ChatGPT 这种全能型选手，需要不断优化训练模型的算法，准备超巨量的数据，提升数据的质量，最后再采购大量的显卡进行实际的训练。在 OpenAI “钞能力”的加持下，翻过了优化算法、准备数据、采购显卡的多重大山，最终将现在超级好用的 ChatGPT 才呈现在了我们的眼前。