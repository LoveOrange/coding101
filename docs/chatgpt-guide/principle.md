---
sidebar_position: 2
---

# ChatGPT 以及 LLM 原理浅析

ChatGPT 是一种大语言模型（Large Language Models，简称 LLM），它是一种**预测型模型**。

简单来说，预测型模型就是根据已有的文字，预测接下来最可能出现的文字。为了实现这一点，需要在训练过程中提供大量数据，LLM 会学习语言的 *基础知识*、*语法规则*、*事实知识* 和 *语言之间的关联* 等，这一步被称为「**预训练**」。在 LLM 掌握了基本的语言知识后，还可以通过「**微调**」步骤，用特定的训练素材帮助它更好地理解特定的场景和需求。

举个例子，如果我们想训练一个爱吃苹果的 LLM。首先，我们把所有能找到的中文素材喂给它，帮助它学习中文的语法和知识，这就是预训练。接下来，我们用特定的数据对它进行微调，不断地告诉它：“苹果好吃”、“吃苹果健康”、“我爱吃苹果”。这样，当我们问它什么水果好吃时，它就会倾向于回答：“苹果好吃”。

当然，这只是一个简单的例子。如果要训练像 ChatGPT 这样全能的模型，需要不断优化训练算法，准备海量高质量的数据，并使用大量显卡进行实际训练。在 OpenAI 的支持下，经过优化算法、准备数据和采购显卡等多重努力，最终呈现出了现在超级好用的 ChatGPT。

这样一来，任何人都可以使用 ChatGPT 进行有趣的对话，无论你是新手还是专家，都能轻松理解和使用它。